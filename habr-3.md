#Clickhouse and Full Text Search

Clickhouse - это аналитическая (OLAP) СУБД.  Хорошо себя показывет, если вам надо много читать много (точнее очень много) данных с диска и аггрегировать их. 

<details>
  <summary>Spoiler warning</summary>
  
  Spoiler text. Note that it's important to have a space after the summary tag. You should be able to write any markdown you want inside the `<details>` tag... just make sure you close `<details>` afterward.
  
  ```javascript
  console.log("I'm a code block!");
  ```
  
</details>

Мне Clickhouse нравится динамикой своего развития, и множеством интересных технологий, собираемых под общим зонтиком высокопроизводительной MPP системы.  В этой статье я покажу как выглядит код на SQL диалекте Clickhouse, и расскажу как можно работать с текстовыми данными на примере индексации википедии и поиска в ней.  

Нужно понимать, что любой инструмент для полнотекстового поиска - это не волшебная палочка, а скорее швейцарский нож - нечто, предоставляющее разработчку возможности решить поставленную перед ним задачу.  И часто - эта задача не похожа на задачу, которую решают гугл и яндекс.  Поэтому настройка даже  хорошего инструмента под конкретные данные и конкретную задачу может оказаться очень непростой.

Clickhouse в том виде, как я наблюдаю его сейчас - это всё ещё NoSQL (not-only-sql)  СУБД, сравнимая скорее с чем-то типа монги, чем с oracle/postgress.  И дело даже не в наличии или отсутствии SQL диалекта, колоночности, транзакциях и прочем OLAP/OLTP.  Дело в том, что в полноценной реляционной SQL СУБД  разработчик во многом полагается на оптимизатор запроса.   

Разработчик создает схему базы данных - описание сущностей, реляций между ними, констрейнты, индексы итп.  Опираясь на эту информацию СУБД решает как лучше выполнить запрос - что делать сначала, а что потом,  какие индексы стоит применять в данном конкретном запросе, какие нет, а какие стоит построить.

В типичной NoSQL СУБД (типа монги или редиса) вы сами описываете весь алгоритм запроса на подходящем языке программирования, обращаясь к функциям, предоставляемых ядром СУБД.  Нужна статистика или понимание кардинальности данных? Сами считаете и сами применяете разные алгоритмы.   Нужны индексы - строите их. Нужен выбор с индексом или без - сами принимаете решение во время запроса. Оптимизатор запросов - сложная штука. Он сам собой не появится, даже если доступ к функциям ядра будет предоставлен не через библиотеку для golang/java/python/etc, а через функции замысловатого SQL диалекта.  

Такова цена свободы.  Вы соглашаетесь на ограничения и можете позволить себе не думать как оно там все работает внутри. В идеальном мире. Однако идеального варианта пока ни у кого не получилось сделать хорошо - если запрос оперирует большими данными, то думать о внутреннем устройстве все-таки приходиться.  Даже если у вас Oracle или Postgress.

В Clickhouse  зачатки оптимизатора только начали появляться. Диалект SQL все ещё далек от ANSI SQL, хотя несколько шагов вперед в прошлом году было сделано (к примеру Window Functions). Поэтому для сохранения душевного спокойствия, лучше считать что оптимизатора и ANSI SQL пока нет, и мы работаем с NoSQL СУБД при помощи специфичного диалекта.

Тем не менее в роадмапе на 2022-й  записано очень много наболевшего, о чем пользователи давно просили Миловидова: 

оптимизация Join запросов

начальный уровень поддержки транзакций

легкие update/delete

приближение к стандарту SQL 

Роадмап - это не чейнджлог (как обещать - не значит жениться).  Однако надеемся на лучшее, и что к середине 2023-го эти красоты будут доступны для production enviroments.

Но пока мы там, где мы есть, и работаем на том Clickhouse, который есть в наличии.  

Пару тезисов из теории баз данных

Если вы хотите разобраться как работает Clickhouse, то стоит прочитать книгу Мартина Клеппмана  «Высоконагруженные приложения. Программирование, масштабирование, поддержка» с кабанчиком на обложке. Не хочу её пересказывать - она прекрасна. Тем не менее, я позволю себе вставить в эту статью несколько  важных тезисов, нужных для дальнейшего восприятия.

Транзакционные vs Аналитические

Есть два известных баззворда ведущих свою историю от Эдгара Кодда - OLTP/OLAP, что расшифровывается как Online Transaction/Analitical Processing. Эти аббревиатуры описывают два разных подхода к работе с данными.

В концепции OLTP  вы хотите достать из базы ровно то, что нужно отдать пользователю, и делать это 10k раз в секунду. В OLAP вы это делаете уже не так часто, зато приходится доставать очень много строк, потом как-то их аггрегировать, чтобы отдаеть аналитику разумное количество.  Поэтому OLTP системы оптимизированы на целевое чтение только нужных данных, а OLAP - на потоковое чтение больших объемов. В результате для OLTP систем нормальное время ответа - единицы миллисекунд, а для OLAP можно пожертвовать скоростью, главное чтобы результат был получен за разумное время. 

Разумность определяется задачей - кому-то надо успеть за 1 сек, а кому-то и к утру нормально. Платить за миллисекундную латенси в аналитических задачах почти никто не хочет.

Индексы и колонки

Если у вас данных мало (200-300-400Gb), то есть шанс применить InMemory СУБД и обращаться к ним напрямую и очень быстро.  Но если данных много, то они лежат на "дисках" (чтобы мы не вкладывали в эти слова сегодня - SSD, HDD, NVMe). И для того, чтобы быстро работать, нам надо меньше читать с диска.  Если мы находимся в мире SQL, то данные у нас представлены 2-х мерными таблицах. Поэтому оптимизация состоит в том, чтобы читать с диска только нужное для ответа - нужные строки и нужные столбцы.

Читать меньше столбцов позволяет колоночная структура хранения - мы разбиваем нашу таблицу не по строкам (как в типичных OLTP), а по столбцам. В Clickhouse каждая колонка хранится в виде отдельного файла.  Вы не тратите драгоценные IOPS на чтение ненужных полей в строке.  Они просто лежат на диске и совсем не мешают.

В традиционных реляционных СУБД проблема "лишних колонок" не очень актуальна, т.к. данные сильно нормализованны - разбиты на множество табличек со связями/реляциями.  В аналитических СУБД часто применяют  денормализацию - данные хранятся уже связанные, в виде очень широких таблиц (количество колонок может измеряться сотнями).  Колоночное хранение позволяет получать только нужные поля, без издержек на чтение с диска полной строки.

Читать меньше строк позволяет индекс.  Это структура, которая занимает место и уменьшает скорость вставки новых данных, зато позволяет обращаться к нужным данным "напрямую", не просматривая всю таблицу.

Колоночное хранение и денормализация позволили пожертвовать привычными всем индексами (dense index).  В Clickhouse применяется т.н. разреженный индекс (sparse index [2]). В отличии от привычных B-tree индексов, которые приводят вас сразу к нужной строке (или строкам) таблицы, индекс пропуска данных позволяет узнать есть ли нужные данные в достаточно большом блоке на диске, и стоит ли его читать.

Если ваша задача "транзакционная", то обычно нужно получить в результатах запроса "мало" строк.  Если же задача "аналитическая", то очень часто вы обращаетесь к значительной части своих данных, поэтому нет смысла гоняться за каждой отдельной строкой - это только замедляет инсерты, и не дает особого выигрыша на селектах.

MergeThree and MPP

Основная стуктура хранения данных в Clickhouse называется MergeThree.

«Изначально эта индексная структура была описана Патриком О’Нилом и др. под названием журналированного дерева слияния (Log-Structured Merge-Tree, LSM-Tree), на основе более ранней работы, посвященной журналированным файловым системам» [1]

Новые данные приходят большими батчами, складываются в структуры, похожие на полноценные таблицы (parts/куски), background процесс потом неспешно их сливает вместе в более крупные куски. Куски всегда иммутабельны, блокировок и конфликтов нет.  В процессе слияния кусков, данные сортируются по первичному ключу, а в некоторых случаях ещё и аггрегируются. Обращения по первичному ключу всегда быстрые - есть индекс. Но разреженный.  По умолчанию элемент индекса содержит ссылку на 8192 строк (можно менять и есть нюансы).  

Идея слияния данных, выдвинутая в статье О’Нила, пронизывает всю внутреннюю архитекутру Clickhouse.  Именно через слияния реализуется функционал Massive Parallel Processing - MPP.  Данные читаются с диска из разных кусков (parts) независимыми тредами, выполняющимися на разных ядрах одного или нескольких серверов. Считанные данные обрабатываются, делается промежуточная аггрегация, а для получения финального результата данные сливаются вместе на каком-то одном сервере, после чего производится финальная аггрегация.  

На мой взгляд  Online MPP - это одна из самых ценных фич Clickhouse. Очень непривычно, что инструмент с таким функционалом отдали в opensource, а не берут немерянных денег [2].

Наличие MPP приводит к особенной модели разработки приложений - вы не можете  пройтись циклом по некоторой таблице - обработка идет паралельно в несколько тредов. Но это не является проблемой, если вы разрабатываете приложения на SQL - он спроектирован ровно для этого. Однако при использовании множества интересных и полезных функций, имеющихся в Clickhouse, разработчик сталкивается с определенными ограничениями, которые накладывает эта паралельность.

Функциональное программирование

В  настоящее время в Clickhouse доступно 1162 функции:

Есть обычные функции, есть математические, есть статистические. Можно обращаться к другим серверам баз данных, можно читать файлы с диска или с веба, можно работать с json или парсить страницы через regexp.  Есть даже функции машинного обучения.  Пользуясь этими функциями можно писать полноценные программы как на любом другом языке программирования.   

Однако есть нюансы, вытекающие из паралельной обработки - т.к. данные иммутабельны, то нет и не может быть переменных в их традиционном понимании. Ни глобальных, ни локальных. Зато есть лямбда-выражения.  Так мы приходим к подходам, принятым в функциональных языках программирования (тут была бы уместна ссылка на прекрасную хабра-статью, но я быстро не нашел достойную, буду рад получить в комментах или ЛС).

В Clickhouse модель ФП достаточно понятная, и ориентирована во многом на работу с массивами:

Функция получает данные доступных типов данных (включая массивы и проч), что-то с ними делает и возвращает данные доступных типов. 

Функция не может ничего менять, только вернуть вычисленное значение.

Если вам потребовалось итерировать один или несколько массивов, то надо написать функциональное выражение (та самая лямбда) и передать в нее эти массивы.  

groupArray - собирает массив из значений в строках. arrayMap - итерирует массив, порождая новый массив.

Можно и посложнее. Давайте сделаем простой токенайзер со стеммингом:

Читаем вызовы функций изнутри:  после приведения к lower case, делим исходную строку при помощи regexp на отдельные слова. В данном случае используется разделитель  '\P{L}+' обозначающий НЕ символы слов с учетом UTF8 символов. Затем применяем фильтр - выкидываем слова из одного символа, а потом снова проходим циклом, порождая массив слов с приведенным к единообразию окончанием. 

Для работы с regexp в Clickhouse включена гугловская библиотека re2 . Там есть некоторые ограничения (например отсутствует backreference), зато она работает быстро и безопасно.

Стеммирование производится соответствующей функцией из недавно появившегося набора NLP функций. Леммаризация там тоже есть, но мне не понравилось как она работает. Набор NLP функций пока находится в статусе экспериментального, и его необходимо явно включать в запросах отдельной настройкой.

В примере выше мы видим повторения слов.  Это не всегда удобно, поэтому текст для анализа превращают в числовой вектор (векторизируют).   работы с текстовыми массивами данных часто применяется концепция Bag Of Words - список уникальных слов, часто со счетчиками.   Bag of Words (BoW) Term Frequency

Берем массив слов с повторениями из примера выше, итерируем по нему, делая массив мапов вида {'a':1}, и отдаем его функции arrayReduce, которая выполняет аггрегацию,  схлопывая весь массив в одно значение, применяя аггрегирующую функцию sumMap ко всем элементам массива.  Похожая операцию делается в SELECT sum()... GROUP BY со строками группы.  Функция sumMap складывает счетчики для каждого отдельного ключа, и выдает в качестве значения объект типа Map.

Выглядит страшновато?   Осенью 2021-го наконец-то появились UDF (User Defined Functions). Теперь можно эту многоступенчатось спрятать в свою собственную функцию, а селект будет выглядеть красивее и понятнее:

Такой SQL компилится при помощи встроенного LLVM в полноценный бинарный код процессора и работает быстро.  UDF функции не вызываются, а инклюдятся на нужное место.  Фактически это не вызов функции, а "синтаксический сахар".  

Полнотекстовый поиск

Если  ваша задача - поиск товаров на сайте, то вам не нужен Clickhouse, надо взять специализированные инструменты типа  ELK или Manticore. 

Нет смысла сравнивать Clickhouse и ELK - это разные инструменты для разных задач.  Тем не менее, я уже не раз видел вопросы от людей, которые переходят или пытаются уйти с ELK в задаче с обработкой логов.  Основная боль - непомерные затраты на хранение огромных объемов данных.  Если фокус задачи прежде всего в эффективном хранении, а поиск делают только свои сотрудники, и Clickhouse уже эксплуатируется в компании, то почему бы и не попробовать.

Нужен датасет. Для начала находим на хабре подходящую статью, где сравнивают ELK и его конкурентов на основе анализа выжимок из английской Википедии (сейчас это 6.45М строк), а потом  сражаются за миллисекунды.  Наверное для поиска на сайте это важно.

С данными будем работать только средствами Clickhouse, без привлечения дополнительных инструментов.  

Для загрузки данных воспользуемся Format Template

ins.sql:

resultset.format:

row.format:

Посчитаем количество вхождений какой-нибудь двухсловной фразы, скажем "George Gershwin" в данном датасете самым простым и очевидным способом: 

Находим 100 вхождений за 140-180 мс. Не очень быстро в сравнении с ELK/Manticore, однако вполне удовлетворительно для аналитического запроса. Нечего тут оптимизировать.  Нужен датасет побольше.

Это же полный перебор датасета, сравнимый с тем что делает обычный grep (у него 2 сек. по некомпрессированному файлу).

Помогает колоночное хранение (меньше читаем ненужного) и MPP - паралельная обработка. Clickhouse читает в 8 потоков, и имеет на диске достаточное количество "кусков" (parts), чтобы загрузить данными все потоки.  Тот же самый запрос, выполненный в 1 поток выполняется за 800 мс.  

Индексируем Википедию

Пришло время загрузить все статьи англоязычной википедии. Их тоже 6.45×106 как и в предыдущем примере, однако слов в полной статье существенно больше чем в абстрактах. Так-же есть страницы (pages) - в википедии одна статья имеет несколько страниц, с которых идет передресация на основную.  Их существенно больше,  и у них есть свои собственные комментарии. 

Wikimedia Foundation отдаёт статьи в виде одного XML файла на ~20Gb с компрессией bz2 (86Gb без компрессии). В отличии от предыдущего варианта, вариабельность XML схемы полной статьи несколько выше, поэтому фиксированными шаблонами парсить не получится.  

Попробуем regexp.  Получим нужные нам данные парсингом XML выгрузки непосредственно в SQL коде, без привлечения дополнительных инструментов. 

Предобработка. Для импорта данных в Clickhouse нужно чтобы одна статья википедии была представленна одной строкой текстового файла (с \n  в конце). Учитывая, что входной файл - это многострочный XML, потребовалась небольшое преобразование (выкидываем все \n, но после каждой </page> добавляем его: 

Количество строк в одном батче должно быть достаточно большим, но не слишком большим, чтобы не занимать черезмерно много RAM. Поэтому разбиваем при помощи split этот монструозный файл на несколько файлов по 50к строк, и грузим их последовательно. Или паралельно, если есть кластер из нескольких серверов. 

Парсинг на regexp, как обычно, отлаживаем на сайте https://regex101.com/ и вставляем в SQL код. Помним про ограничения, накладываемые библиотекой re2.

Инсертим текстовые данные в Null таблицу (которая ничего не хранит), с последующей обработкой непосредственно на SQL внутри Materialized View. Мне так удобнее - код, который обрабатывает входные данные находился внутри базы. Это можно сравнить со stored procedure для Oracle/Postgress.  

При вставке блока (50 000 строк) в таблицу onerow, срабатывает триггер материализованного представления. Блок обрабатывается указанным выше запросом.  Результаты вставляются в таблицу wiki_pages.

Индекс пропуска данных

Дополнительные индексы пропуска данных не всегда работают так, как хочется разработчику.  Они не приносят немедленного счастья и ускорения на произвольных данных.  Особенно на строковых. 

Поэтому мы эти данные немного приготовим, используя встроенные в Clickhouse функции для работы со строками и словами. Извлечем токены из каждой статьи в формат Bag-of-Words, используя написанный выше токенизатор, и построим bloom_filer по спискам токенов.

alters etc

Ищем как часто встречается слово Германия в одной статье с Россией. Для формирования условия используем функцию поиска массива в массиве hasAll.  

Единицы секунд.   Для аналитика, меланхолично вгрызающегося в данные, это нормальная скорость. Особенно если и более сложные запросы будут работать примерно с тем-же временем ответа. Однако для нервного посетителя какого-нибудь маркета, покупающего не слишком нужный ему товар, такое время ответа не пойдет.

Посмотрим насколько нам помог индекс:

Первичный индекс не ограничил ничего (что понятно), Skip индекс уменьшил объем просмотра в 3 раза.  

Гранула  у нас  была 1024 строки. Можно подумать что слова встретились в каждой третьей грануле. Но нет - bloom_filter похож по принципу работы на хеш функцию с большим количеством коллизий.  Все коллизии уходят в сторону false positives - мы ничего не потеряем, но просмотрим лишнего. Вот и получилось, что наш запрос за 5 сек. прошелся по 1/3 датасета.  

На мой взгляд все-таки тормозит. Надо быстрее.

Inverted Indexes

Как было сказано в начале статьи, в Clickhouse не стали делать индексы, точно указывающие на отдельную строку.  Однако, если в этом есть потребность, то их не трудно сделать самостоятельно, в виде  дополнительной таблички.  Главное чтобы был в наличии id строки.  Не всегда это просто - в Clickhouse для обеспечения паралельности работы по тредам и серверам кластера пришлось пожертвовать генератором последовательностей, который есть во всех транзакционных СУБД. Однако в случае с Википедией, id  у нас есть непосредственно в исходных данных.  

Делаем таблицу соответствия - токен к id страницы, где он встретился.  Заодно сохраним количество токенов на указанной странице - поможет при ранжировании.

Для каждой статьи википедии есть два текстовых массива - сама статья и комментарии к ней.  Поэтому делаем две отдельные таблицы - в этой задаче лучше разделять, чем объединять.

Пользуясь такими дополнительными таблицами мы можем быстро (за 5-50 миллисекунд)  найти статью, в которых упоминался интересующий нас токен или токены.  К сожалению, таких статей может найтись очень много, и эта масса документов затормозит наш поиск и поставит перед выбором - какой документ более подходящий и что отдать в результатах запроса.  Однако для подсчета статей этой проблемы нет.

Заголовки статей в которых встречается слово Германия:

Ранжирование и SummingMergeThree

В нашем учебно-поясняющем примере мы все ближе к настоящей поисковой системе.   Быстро найти и выдать случайную статью - достаточно просто.  Сложно найти подходящую.  Эту задачу решают алгоритмы ранжирования результатов поиска. Тема ранжирования бесконечна, углубляться туда не будем, а попробуем применить что-нибудь не очень сложное для восприятия и реализации.  Моя задача показать инструментарий Clickhouse  и базовые понятия поиска, а не изобретать велосипед строить полноценную поисковую систему.

Есть очень старый алгоритм (или даже метод) для подсчета частотностей слов в отдельном документе и полном наборе.  Он называется TF-IDF и ведет свою историю от двух независимых работ, сделанных в разное время - в 1957 г. Hans Peter Luhn предложил учитывать частотность слова в документе (Term Frequency) как отношение количества вхождений слова к общему количеству слов в документе.  А в 1972 г.  Karen Spärck Jones предложил пессимизировать очень часто встречающиеся слова, учитывая их количество во всех документах массива данных  (Inverse Document Frequency).

 Давайте посчитаем эти частотности.

Мы не зря накапливали счетчики слов в Bag-of-Words -  на их основе можно посчитаеть TF.  Сохраним их для будущего ранжирования вместе с количеством токенов в статье:

при вставке очередного блока в таблицу wiki_pages, срабатывает триггер материализованного представления и выполняется указанный запрос - только над этим блоком, а не всей таблицей.

mapKeys делает массив ключей, а mapValues - значений.  Длины этих массивов конечно же одинаковые.

array Join размножает строки таблицы итерируя массив - для каждой статьи википедии временно создается много строк, по одной на каждое уникальное слово. 

wiki_text содержит очень много строк (произведение количества статей на количество токенов в них). Т.к. она отсортирована по токену, то выборки  по токену  быстрые.

Для вычисления IDF нужно посчитать количество документов в которых встречается каждый токен.  Для этого придется создать ещё одну табличку, в которой будем накапливать поступающую при индексации информацию.  Т.к. в Clickhouse все структуры хранения иммутабельны, то у нас нет возможности делать UPDATE. Все что мы можем делать - вставлять новые строки в таблицу. 

Как же тогда решается задача накопления числовых данных в счетчиках?

В Clickhouse есть несколько "хитрых" Table Engine, которые в процессе мерджа кусков (parts) делают аггрегации данных.  Строк становится меньше, а выборки - быстрее.   Один из таких "движков"  SummingMergeThree  "схлопывает" все числовые значения в полях вне первичного ключа, используя аггрегирующую функцию sum().  Нужно учесть, что нет гарантий окончания процесса  -  надо не забыть сделать финальную аггрегацию при выборке данных.

Таким образом, для каждого поступающего батча данных (они у нас по 50к страниц), мы делаем первичную аггрегацию - считаем уникальные id статей на каждое слово и вставляем эти числа в таблицу SummingMergeThree. В рамках последовательных мерджей количества id на слово суммируются. Мы можем так поступать, потому что точно знаем что в каждом батче мы получаем новые id. Финальная аггрегация тоже содержит вызов sum() и считает IDF для данного конкретного токена.

Считаем количество статей на слово

Заполняем данными

Считаем IDF

token

idf

and

2.86

the

3

or

5.47

russia

7.83

gershwin

12.26

Все готово.  Теперь можно поискать статьи, наиболее релевантные запросу 'George Gershwin':

link

tfidf

BM25

sc2

George Gershwin

2.317

0.335

6.751

Category:Compositions by George Gershwin

7.501

21.009

6.098

List of Liberty ships (G–Je)

1.925

0.456

5.169

Category:Operas by George Gershwin

7.313

21.98

4.403

Second Rhapsody

1.907

1.068

4.363

List of compositions by George Gershwin

1.613

0.415

4.283

Category:Songs with music by George Gershwin

5.319

17.491

3.938

Gershwin (disambiguation)

2.373

4.434

3.802

An American in Paris

1.346

0.339

3.587

Sarah Vaughan Sings George Gershwin

1.736

1.793

3.433

Ira Gershwin

1.268

0.375

3.287

The Complete Ella Fitzgerald & Louis Armstrong on Verve

1.177

0.568

2.784

Rhapsody in Blue

0.941

0.127

2.772

The Man I Love (song)

1.32

1.191

2.713

Frances Gershwin

1.317

1.307

2.64

Catfish Row

1.127

0.68

2.544

Naughty Baby (album)

1.163

0.908

2.478

Gershwin Live!

1.182

1.116

2.404

Cuban Overture

1.06

0.672

2.368

Arthur Gershwin

1.13

0.949

2.366

Время выполнения 140-150 мс.  Меня устраивает.

TF-IDF отработал не так уж и плохо, но мне пришлось сделать коррекцию, чтобы  дать приоритет более длинным статьям (наверное они ценнее?).  Вроде бы для этого придумали BM25, но он меня не особо впечатлил. Совершенстовать ранжирование можно до бесконечности. Лучше всего на тех данных, которые есть, и на тех пользователях, которые что-то хотят найти.  Какой бы инструмент вы не выбрали для поиска, настраивать ранжирование "под себя" все равно придется. Можно посмотреть как в документации Manticore объясняются доступные там алгоритмы ранжирования и их параметры. 

сравнили занимаемое место

обратили внимание на размер колонок

смотрим на занимаемое время и на скорость загрузки

смотрим на время поиска

Переиндексация и VersionedCollapsingMergeTree

Исходный набор данных постоянно изменяется.  За ними должен следовать и индекс.  Старые слова и ссыки на документы надо удалять, новые добавлять.  В википедии для этого всё предусмотрено - в файле экспорта у каждой страницы есть не только индивидуальный id, но и номера текущей и предыдущей версии. Пользуясь номерами версий можно найти что нужно удалить из индекса.

Т.к. в Clickhouse все структуры данных иммутабельные, то вы не можете просто так что-то удалить. Так-же как и с обновлениями, можно дописать в таблицу новые строки.  Когда-нибудь в результате мерджей старые, ненужные строки будут удалены.  Место сократится, запросы ускорятся.  Но всегда нужно помнить, что в таблице есть старые строки и правильно фильтровать их при выборке. 

Для чистки основной таблицы документов и инверсных индексов применяем  вариант MergeThree под названием VersionedCollapsingMergeTree. При вставках новых документов делаем дополнительный insert с Sign = -1  и ставим revision_parent в поле revision_id 

При добавлении измененной версии документа мы добавляем не только его, но и старый документ с обратным знаком.  Выбирать будем только самые новые версии документа,  а старые будут удалены когда-нибудь при очередном мердже.

Для счетчиков встречаемости слова, вставляем токены удаляемых документов похожим образом, но с отрицательным знаком.

Ссылки для самостоятельного изучения:

документация

clickhouse inc

altinity блог

altinity kb

видосики на youtube

статья Денни про MV

На хабре есть отличная статья с деталями и подробностями о MergeThree

